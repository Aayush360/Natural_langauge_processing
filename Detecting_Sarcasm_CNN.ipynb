{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting_Sarcasm_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPas7C6itkC3rbAaioDhmlb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush360/Natural_langauge_processing/blob/main/Detecting_Sarcasm_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MXPnE5OCTX0"
      },
      "source": [
        "# import libraries"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdtfbduFCbgs",
        "outputId": "f716902b-b32a-427f-bf3d-1432e2a5a9c3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import gensim\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential,Model\n",
        "from keras import layers\n",
        "from keras.layers import Dense,Dropout,Conv1D,GlobalMaxPooling1D\n",
        "\n",
        "import h5py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvLUojsLDWS_"
      },
      "source": [
        "def parse_data(file):\n",
        "  for l in open(file,'r'):\n",
        "    yield json.loads(l)\n",
        "\n",
        "data = list(parse_data('Sarcasm_Headlines_Dataset_v2.json'))\n",
        "df = pd.DataFrame(data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3QbWbG2lHuBz",
        "outputId": "380215ce-3f36-439a-97a7-ec53bc3a759e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnW1IcI_HwWn"
      },
      "source": [
        "# This is primarily data from news headlines sourced from The Onion and the Huffington Post."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-eIn8XIFLI"
      },
      "source": [
        "# article link is not much used in our analysis so we can remove it"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MN1nYqmjIOLD",
        "outputId": "a6e9161d-91f6-4461-cd6b-cd6e14b7cdc5"
      },
      "source": [
        "df.pop('article_link')\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomething scientists unveil doomsday clo...\n",
              "1             0  dem rep. totally nails why congress is falling...\n",
              "2             0  eat your veggies: 9 deliciously different recipes\n",
              "3             1  inclement weather prevents liar from getting t...\n",
              "4             1  mother comes pretty close to using word 'strea..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FGEaBwOISJ0",
        "outputId": "e248cb68-4d70-4b4a-d660-c0b9e6520fe1"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X1cRrw_1aSC",
        "outputId": "aad0033f-541f-4d61-9f9b-b6f76b369c1c"
      },
      "source": [
        "df['is_sarcastic'].unique() # so there are only 2 classes"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0DNWNdcIVMl"
      },
      "source": [
        "# there are 28619 instances of dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZImW6NEcIsmX"
      },
      "source": [
        "# Data Preprocessing"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sBkY-slIbs9"
      },
      "source": [
        "# let us clean our data:\n",
        "# remove special characters\n",
        "# keep only alphanumeric data\n",
        "# remove stop words\n",
        "# lemmatize our data\n",
        "# perform case-folding on the data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKqhJuW71nrI"
      },
      "source": [
        "def text_clean(corpus):\n",
        " \n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "            p1 = p1.lower()\n",
        "            qs.append(p1)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UxqP9yK1wWU"
      },
      "source": [
        "def stopwords_removal(corpus):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33b-2v7115bc"
      },
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XZBXjos18dK"
      },
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdOf9oSZ1_i4"
      },
      "source": [
        "def preprocess(corpus, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    \n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    \n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus\n",
        "    \n",
        "    '''\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus)\n",
        "    \n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "    \n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "        \n",
        "        \n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "    \n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "        \n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX5Ex3Uq2Gxx",
        "outputId": "75b79c1c-892d-43a6-cff4-2dadc3b7fe7e"
      },
      "source": [
        "# text we want to clean is :\n",
        "df['headline']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        thirtysomething scientists unveil doomsday clo...\n",
              "1        dem rep. totally nails why congress is falling...\n",
              "2        eat your veggies: 9 deliciously different recipes\n",
              "3        inclement weather prevents liar from getting t...\n",
              "4        mother comes pretty close to using word 'strea...\n",
              "                               ...                        \n",
              "28614         jews to celebrate rosh hashasha or something\n",
              "28615    internal affairs investigator disappointed con...\n",
              "28616    the most beautiful acceptance speech this week...\n",
              "28617    mars probe destroyed by orbiting spielberg-gat...\n",
              "28618                   dad clarifies this not a food stop\n",
              "Name: headline, Length: 28619, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E682isZo2MtC",
        "outputId": "31727134-5bd7-43dd-c791-f7e442aaa54a"
      },
      "source": [
        "headlines = preprocess(df['headline'], lemmatization=True, remove_stopwords=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYSuZ9J2fiv",
        "outputId": "0d2f38e6-46af-4f7a-9224-e027d043c63d"
      },
      "source": [
        "headlines[:4]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thirtysomething scientists unveil doomsday clock hair loss',\n",
              " 'dem rep totally nail congress fall short gender racial equality',\n",
              " 'eat veggies 9 deliciously different recipes',\n",
              " 'inclement weather prevent liar get work']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJAslefQ2oos"
      },
      "source": [
        "# now that we have performed cleaning of the corpus we are ready to train our model\n",
        "# this is our pretrained Word2Vec model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3payyB2j7AQn",
        "outputId": "ba59f8d6-83d9-44dd-fd69-09c103abe6f6"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 13:58:13--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.171.104\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.171.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  86.2MB/s    in 22s     \n",
            "\n",
            "2021-04-29 13:58:35 (72.2 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMlRxgU72vNy"
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBUqMvDr7GPH"
      },
      "source": [
        "# now we will be seding the word_vectors each of size 300, to the CNN model\n",
        "# we need each of our data(headlines) to be standarized i.e should have eaxacly 10 words, if more truncate is less pad 0's in the vecotr to make lenght 300"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyAgyz4f3QRe"
      },
      "source": [
        "MAX_LENGTH=10\n",
        "VECTOR_SIZE=300"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XPTzEeL73VR"
      },
      "source": [
        "def vectorize_data(data):\n",
        "  vectors=[]\n",
        "  padding_vector = [0.0]*VECTOR_SIZE\n",
        "\n",
        "  for i,data_point in enumerate(data):\n",
        "    data_points_vector = []\n",
        "    count=0\n",
        "    tokens = data_point.split()\n",
        "\n",
        "    for token in tokens:\n",
        "      if count>=MAX_LENGTH:\n",
        "        break\n",
        "      if token in model.wv.vocab:\n",
        "        data_points_vector.append(model.wv[token])\n",
        "      count=count+1\n",
        "    \n",
        "    if len(data_points_vector)<MAX_LENGTH:\n",
        "      to_fill = MAX_LENGTH -len(data_points_vector)\n",
        "      for _ in range(to_fill):\n",
        "        data_points_vector.append(padding_vector)\n",
        "    \n",
        "    vectors.append(data_points_vector)\n",
        "  return vectors\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgQ4JYm98AVT",
        "outputId": "699a3dc4-c532-4288-9724-83eb63aa0db4"
      },
      "source": [
        "\n",
        "model.wv['hello'].size # array/vector of length 300"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUGgAGhi8CMP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27SLO2hl8992",
        "outputId": "776829d8-0ca3-4432-9752-9744520f900e"
      },
      "source": [
        "headlines[:4]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thirtysomething scientists unveil doomsday clock hair loss',\n",
              " 'dem rep totally nail congress fall short gender racial equality',\n",
              " 'eat veggies 9 deliciously different recipes',\n",
              " 'inclement weather prevent liar get work']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUFpHCda-X-3",
        "outputId": "c0db0ea6-4988-4565-d352-4c6a4d8482bd"
      },
      "source": [
        "vectorized_headlines = vectorize_data(headlines)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TQKnAnl-fdd",
        "outputId": "49c98cea-3272-42db-ad81-c5ac825ee709"
      },
      "source": [
        "len(vectorized_headlines[0])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYcQXZSI-lDZ",
        "outputId": "b4dde386-13af-4cdc-8044-ec328eb32b50"
      },
      "source": [
        "vectorized_headlines[0][6].size"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbM3hK75-3Ez"
      },
      "source": [
        "# validation to ensure that the 10 vectors are present for each headline,"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDTmlbEX_BtF"
      },
      "source": [
        "for i, vec in enumerate(vectorized_headlines):\n",
        "  if len(vec)!=MAX_LENGTH:\n",
        "    print(i)\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqGcZGkNAB8J",
        "outputId": "bf06b503-a65f-4ea4-c078-b3e9e8ad4a21"
      },
      "source": [
        "len(vectorized_headlines)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_7QWHyHALfU"
      },
      "source": [
        "# splitting the data into trian and testset"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bWM_WxNBCgp",
        "outputId": "f11cbaf5-f355-4e6b-c6eb-0c67d1353c96"
      },
      "source": [
        "train_div = math.floor(0.7*len(vectorized_headlines))\n",
        "train_div"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xr2Xj_BOdJ"
      },
      "source": [
        "X_train = vectorized_headlines[:train_div]\n",
        "y_train = df['is_sarcastic'][:train_div]\n",
        "X_test = vectorized_headlines[train_div:]\n",
        "y_test = df['is_sarcastic'][train_div:]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyMAFMXGBipu",
        "outputId": "97e4512f-4c06-4520-e749-44fb9d9c2e6f"
      },
      "source": [
        "print('training data size is ',len(X_train))\n",
        "print('test data size is',len(X_test))\n",
        "print('trian label size is', len(y_train))\n",
        "print('test label size is', len(y_test))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size is  20033\n",
            "test data size is 8586\n",
            "trian label size is 20033\n",
            "test label size is 8586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCAP6hHaBq4T"
      },
      "source": [
        "# reshaping the data to feed into CNN model"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndkq_avOCHOz"
      },
      "source": [
        "X_train = np.reshape(X_train,(len(X_train),MAX_LENGTH,VECTOR_SIZE))\n",
        "X_test = np.reshape(X_test,(len(X_test),MAX_LENGTH,VECTOR_SIZE))\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuzIorrCCgU7"
      },
      "source": [
        "## Building the model"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMHMOUpCqCU"
      },
      "source": [
        "# definign hyperparameters"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_83HQLuCkUf"
      },
      "source": [
        "FILTERS = 8\n",
        "KERNEL_SIZE = 3\n",
        "HIDDEN1_NODES =10\n",
        "HIDDEN2_NODES = 5\n",
        "DROPOUT = 0.35\n",
        "NUM_EPOCHS=10\n",
        "BATCH_SIZE = 50"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7KHE7QfDDJm"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(FILTERS,KERNEL_SIZE,strides=1,padding='same',activation='relu',input_shape=(MAX_LENGTH,VECTOR_SIZE)))\n",
        "model.add(GlobalMaxPooling1D())"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPCv0a7qDtGR"
      },
      "source": [
        "# we have used 1 dimensional convolutions due to signal dimensionality associated with text data"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtxK2Pq1D26v"
      },
      "source": [
        "model.add(Dense(HIDDEN1_NODES,activation='relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "model.add(Dense(HIDDEN2_NODES,activation='relu'))\n",
        "model.add(Dropout(DROPOUT))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v28q9ONEPK7",
        "outputId": "754746c1-7ba1-4118-b9c1-d7b8d8459196"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 10, 8)             7208      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                90        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 7,359\n",
            "Trainable params: 7,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkWP5gGzEQxu"
      },
      "source": [
        "#We have 7,359 trainable parameters in our model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XDRiPYxEWkV"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vBa4-QKEkSz",
        "outputId": "c7b384d9-2782-40fe-dcc3-457660501da1"
      },
      "source": [
        "training_history = model.fit(X_train,y_train,epochs=NUM_EPOCHS,batch_size=BATCH_SIZE)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "401/401 [==============================] - 4s 8ms/step - loss: 0.6787 - accuracy: 0.5621\n",
            "Epoch 2/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.6197 - accuracy: 0.6877\n",
            "Epoch 3/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.5507 - accuracy: 0.7345\n",
            "Epoch 4/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.5177 - accuracy: 0.7577\n",
            "Epoch 5/10\n",
            "401/401 [==============================] - 3s 8ms/step - loss: 0.4763 - accuracy: 0.7893\n",
            "Epoch 6/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.4530 - accuracy: 0.8103\n",
            "Epoch 7/10\n",
            "401/401 [==============================] - 3s 8ms/step - loss: 0.4355 - accuracy: 0.8204\n",
            "Epoch 8/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.4149 - accuracy: 0.8333\n",
            "Epoch 9/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.3916 - accuracy: 0.8431\n",
            "Epoch 10/10\n",
            "401/401 [==============================] - 3s 7ms/step - loss: 0.3775 - accuracy: 0.8523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTgMabN2Ev5-"
      },
      "source": [
        "# evaluating and saving our model"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR1oNVm9E4Rh",
        "outputId": "c7b6cb6a-478e-4d4f-e952-6f4d670467c0"
      },
      "source": [
        "loss,accuracy = model.evaluate(X_test,y_test,verbose=False)\n",
        "print('test acc is: {:4f}'.format(accuracy))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test acc is: 0.755183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ssNlV4sFHCz"
      },
      "source": [
        "# You can fine-tune various parameters and add/delete layers to obtain other result"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDQUp6d_FKuo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}