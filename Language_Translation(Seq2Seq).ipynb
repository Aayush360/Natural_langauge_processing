{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_Translation(Seq2Seq).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsMho23OONAoOV0FSaPilo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush360/Natural_langauge_processing/blob/main/Language_Translation(Seq2Seq).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4GRWf0pROxe"
      },
      "source": [
        "[Data Source](http://www.manythings.org/anki/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o2a_gk4QX4a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import io\n",
        "\n",
        "from unicodedata import normalize\n",
        "import keras, tensorflow\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM,Dense"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyxPsmG5Ro1U"
      },
      "source": [
        "# reading a data\n",
        "\n",
        "def read_data(file):\n",
        "  data=[]\n",
        "  with io.open(file,'r') as file:\n",
        "    for entry in file:\n",
        "      entry = entry.strip()\n",
        "      data.append(entry)\n",
        "  return data"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz7r8mYzSJbO"
      },
      "source": [
        "data = read_data('bilingual_pairs.txt')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIu0Q2qzTkKe",
        "outputId": "abf8bbbe-3ab2-4fdf-86df-4c4faed55acc"
      },
      "source": [
        "data[139990:140000]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Never choose a vocation just because the hours are short.\\tNe choisissez jamais une profession juste parce que les heures y sont courtes.',\n",
              " \"No other mountain in the world is so high as Mt. Everest.\\tAucune montagne au monde n'atteint la hauteur du Mont Everest.\",\n",
              " \"No sooner had he met his family than he burst into tears.\\tÀ peine avait-il rencontré sa famille qu'il éclata en sanglots.\",\n",
              " \"Nothing is more disappointing than to lose in the finals.\\tRien n'est plus décevant que de perdre en finale.\",\n",
              " \"Now that he is old, it is your duty to go look after him.\\tÀ présent qu'il est vieux, c'est ton devoir de veiller sur lui.\",\n",
              " \"Now that you've decided to quit your job, you look happy.\\tMaintenant que vous avez décidé de quitter votre emploi, vous avez l'air heureux.\",\n",
              " \"Now that you've decided to quit your job, you look happy.\\tMaintenant que tu as décidé de quitter ton emploi, tu as l'air heureux.\",\n",
              " \"Now that you've decided to quit your job, you look happy.\\tMaintenant que vous avez décidé de quitter votre emploi, vous avez l'air heureuse.\",\n",
              " \"Now that you've decided to quit your job, you look happy.\\tMaintenant que tu as décidé de quitter ton emploi, tu as l'air heureuse.\",\n",
              " 'Please drop in when you happen to be in the neighborhood.\\tVeuillez donc passer quand vous êtes dans le coin !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErhNvDWlTwPV",
        "outputId": "ed0c23b4-c73b-481b-9283-af2791868a93"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "145437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU1TLXywUN0l",
        "outputId": "8c004366-135d-476d-e92a-48ebff09600f"
      },
      "source": [
        "data[500:501]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I beg you.\\tJe te prie.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkBPPSgVUS_7"
      },
      "source": [
        "# let us use 140000 english-french sentence pair"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7mcDHnRUejw"
      },
      "source": [
        "data= data[:140000]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYYqFS8TUhAd"
      },
      "source": [
        "# separate data into english french list "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQkl9XTiUrIu"
      },
      "source": [
        "def build_english_french_sen(data):\n",
        "  english_sentences = []\n",
        "  french_sentences = []\n",
        "  for datapoint in data:\n",
        "    english_sentences.append(datapoint.split('\\t')[0])\n",
        "    french_sentences.append(datapoint.split('\\t')[1])\n",
        "  return english_sentences, french_sentences\n",
        "\n",
        "\n",
        "eng_sen, french_sen = build_english_french_sen(data)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luXlTgE9VQmc",
        "outputId": "d01e66b5-a903-4dc2-de07-4e56a6347005"
      },
      "source": [
        "eng_sen[:10]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.',\n",
              " 'Run!',\n",
              " 'Run!',\n",
              " 'Wow!',\n",
              " 'Fire!',\n",
              " 'Help!',\n",
              " 'Jump.',\n",
              " 'Stop!',\n",
              " 'Stop!',\n",
              " 'Stop!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAlU_ltsVUNz",
        "outputId": "264ae95d-f586-4949-ced4-bce8a29c43a4"
      },
      "source": [
        "french_sen[:10]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Va !',\n",
              " 'Cours\\u202f!',\n",
              " 'Courez\\u202f!',\n",
              " 'Ça alors\\u202f!',\n",
              " 'Au feu !',\n",
              " \"À l'aide\\u202f!\",\n",
              " 'Saute.',\n",
              " 'Ça suffit\\u202f!',\n",
              " 'Stop\\u202f!',\n",
              " 'Arrête-toi !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqH3t5NaVY6E"
      },
      "source": [
        "# data cleaning"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZDnl1kAVrGq"
      },
      "source": [
        "def clean_sentences(sentence):\n",
        "  # prepare regex for char filtering\n",
        "  re_print = re.compile('[^%s]'%  re.escape(string.printable)) # removes non-printable characters\n",
        "  # prepare translation table for removing punctuation\n",
        "  table = str.maketrans('','',string.punctuation)\n",
        "  cleaned_sent = normalize('NFD', sentence).encode('ascii','ignore')\n",
        "  cleaned_sent = cleaned_sent.decode('UTF-8')\n",
        "  cleaned_sent = cleaned_sent.split()\n",
        "  cleaned_sent = [word.lower() for word in cleaned_sent] # case-folding\n",
        "  cleaned_sent = [word.translate(table) for word in cleaned_sent]\n",
        "  cleaned_sent = [re_print.sub('',w) for w in cleaned_sent]\n",
        "  cleaned_sent = [word for word in cleaned_sent if word.isalpha()] # keeps only alphabetic word\n",
        "  return ' '.join(cleaned_sent)\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpszpvL1Wwbx",
        "outputId": "1ce924b3-3681-404f-fed8-6c8b0cf77766"
      },
      "source": [
        "re_print = re.compile('[^%s]'%  re.escape(string.printable))\n",
        "re_print"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r'[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"\\#\\$%\\&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\\ \\\\t\\\\n\\\\r\\\\x0b\\\\x0c]',\n",
              "re.UNICODE)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4NpwoKqZW80g",
        "outputId": "372b5281-6ff5-4a89-d77b-d2f398077071"
      },
      "source": [
        "s = \"string. With. Punctuation?\" # Sample string \n",
        "out = s.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "out"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'string With Punctuation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2--c39LOXMpF",
        "outputId": "e9a07c81-bf72-411a-dbe8-0691046dd33e"
      },
      "source": [
        "my_var = \"this is a string\"\n",
        "my_var2 = \" Esta es una oración que está en español \"\n",
        "my_var3 = normalize('NFD', my_var2).encode('ascii', 'ignore').decode('utf8')\n",
        "output = my_var + my_var3\n",
        "print(output)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is a string Esta es una oracion que esta en espanol \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qz13BRE7YyXP",
        "outputId": "967c73a1-3e81-4ca9-e493-01882425cb92"
      },
      "source": [
        "re_print.sub('','/punct,si?s.')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/punct,si?s.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HG-3ae7aXz4"
      },
      "source": [
        "# building clean engish french sentence\n",
        "\n",
        "def build_clean_eng_french_sentence(eng_sent, french_sent):\n",
        "  french_sent_cleaned = []\n",
        "  eng_sent_cleaned = []\n",
        "\n",
        "  for sent in french_sent:\n",
        "    french_sent_cleaned.append(clean_sentences(sent))\n",
        "  for sent in eng_sent:\n",
        "    eng_sent_cleaned.append(clean_sentences(sent))\n",
        "  \n",
        "  return eng_sent_cleaned, french_sent_cleaned\n",
        "\n",
        "\n",
        "eng_sent_cleaned, french_sent_cleaned = build_clean_eng_french_sentence(eng_sen,french_sen)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWFw9Q2yxTdj"
      },
      "source": [
        "# now we should build our vocabulary and add token that convey start and end of a sequence as required by our decoder"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0tqEyG0xpTL"
      },
      "source": [
        "# here instead of dwelling on word level, we will go to character level to build vocabulary"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95WFS3yCx9nV"
      },
      "source": [
        "def build_data(eng_sent_cleaned,french_sent_cleaned):\n",
        "  input_datasets=[]\n",
        "  target_datasets=[]\n",
        "\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "\n",
        "  for french_sent in french_sent_cleaned:\n",
        "    input_datapoint = french_sent\n",
        "    input_datasets.append(input_datapoint)\n",
        "\n",
        "    for char in input_datapoint:\n",
        "      input_characters.add(char) # list of unique input characters\n",
        "    \n",
        "  for eng_sent in eng_sent_cleaned:\n",
        "    target_datapoint = '\\t'+eng_sent+'\\n' # to convey strat and end of the sentence to the decoder\n",
        "    target_datasets.append(target_datapoint)\n",
        "    for char in target_datapoint:\n",
        "      target_characters.add(char) # list of unique output characters\n",
        "  return input_datasets,target_datasets,sorted(input_characters),sorted(target_characters)\n",
        "\n",
        "input_datasets, target_datasets, input_characters, target_characters = build_data(eng_sent_cleaned,french_sent_cleaned)\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nM-lliBzmLN"
      },
      "source": [
        "# input_datasets=[]\n",
        "# input_characters=set()\n",
        "# for french_sent in french_sent_cleaned:\n",
        "#     input_datapoint = french_sent\n",
        "#     input_datasets.append(input_datapoint)\n",
        "\n",
        "#     for char in input_datapoint:\n",
        "#       input_characters.add(char)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCNLATsazqA9",
        "outputId": "3a1da73f-de7d-43b1-f133-5248f9c2ccce"
      },
      "source": [
        "input_datasets[50:54]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hors de question', 'vraiment', 'vrai', 'ah bon']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2pTs9zkzzkF"
      },
      "source": [
        "# for char in input_datapoint:\n",
        "#   print(char)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo2EIhvmz9LQ"
      },
      "source": [
        "# target_datasets=[]\n",
        "# target_characters=set()\n",
        "# for eng_sent in eng_sent_cleaned:\n",
        "#     target_datapoint = '\\t'+eng_sent+'\\n'\n",
        "#     target_datasets.append(target_datapoint)\n",
        "#     for char in target_datapoint:\n",
        "#        target_characters.add(char)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsJEQF8M0XfF"
      },
      "source": [
        "# target_datapoint"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ZrwA7o0ZbA",
        "outputId": "5fa0afbb-f5b2-4c04-996c-2d5adb5d67d5"
      },
      "source": [
        "target_datasets[:4]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\tgo\\n', '\\trun\\n', '\\trun\\n', '\\twow\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukE02lkH0dZr",
        "outputId": "14a57098-f8e1-4eed-d0e7-055a090be3a6"
      },
      "source": [
        "print(input_characters)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyE9tLLo1GW2",
        "outputId": "912656da-7cca-4ce3-e6cf-0fd7f55ef3d5"
      },
      "source": [
        "print(target_characters)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\t', '\\n', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530PH80q1T0I"
      },
      "source": [
        "Our input and output vocabulary may not be the same for tasks such as natural language translation. In fact, at times, our character set may not be the same either. For example, we might be trying to translate between English and Nepali, which have different character sets altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Z_CEaH1KVI"
      },
      "source": [
        "# also, input and output sequence may not be of same length as well"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7I_u7T71ULh"
      },
      "source": [
        "# let us find out some metadata about the data"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frJqt6782ZaB",
        "outputId": "685c211c-ea11-47e0-d36f-4234c3ea4c3d"
      },
      "source": [
        "len(target_characters), len(input_characters)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0mMJbCy25UE"
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIU-YsV11-Xy",
        "outputId": "d8fcb0fa-62ae-4f20-97f1-1c679b4e095d"
      },
      "source": [
        "def build_metadata(input_datasets,target_datasets,input_characters,target_characters):\n",
        "  num_Encoder_tokens = len(input_characters)\n",
        "  num_Decoder_tokens = len(target_characters)\n",
        "  max_Encoder_Sequence_len = max(len(datapoint) for datapoint in input_datasets) # max character length of input sentence\n",
        "  max_Decoder_Sequence_len = max(len(datapoint) for datapoint in target_datasets) # max char length in target sentences\n",
        "  print('Number of datapoints',len(input_datasets))\n",
        "  print('Number of unique input tokens',num_Encoder_tokens)\n",
        "  print('Number of unique target tokens',num_Decoder_tokens)\n",
        "  print('Maxumum sequence length for inputs:', max_Encoder_Sequence_len)\n",
        "  print('Maximum sequence length for outputs: ', max_Decoder_Sequence_len)\n",
        "  return num_Encoder_tokens, num_Decoder_tokens, max_Encoder_Sequence_len, max_Decoder_Sequence_len\n",
        "\n",
        "num_Encoder_tokens, num_Decoder_tokens, max_Encoder_Sequence_len, max_Decoder_Sequence_len = build_metadata(input_datasets,target_datasets,input_characters,target_characters)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of datapoints 140000\n",
            "Number of unique input tokens 27\n",
            "Number of unique target tokens 29\n",
            "Maxumum sequence length for inputs: 117\n",
            "Maximum sequence length for outputs:  58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjTFp_Sc42EA"
      },
      "source": [
        "# build character to indices mapping and vice-versa"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaeU4TVV5Ver"
      },
      "source": [
        "def build_indices(input_characters,target_characters):\n",
        "  input_char_to_idx = {}\n",
        "  input_idx_to_char = {}\n",
        "  target_char_to_idx ={}\n",
        "  target_idx_to_char = {}\n",
        "  \n",
        "  for i, char in enumerate(input_characters):\n",
        "    input_char_to_idx[char]=i\n",
        "    input_idx_to_char[i]=char\n",
        "  \n",
        "  for i,char in enumerate(target_characters):\n",
        "    target_char_to_idx[char]=i\n",
        "    target_idx_to_char[i]=char\n",
        "  \n",
        "  return input_char_to_idx,input_idx_to_char,target_char_to_idx, target_idx_to_char \n",
        "\n",
        "input_char_to_idx,input_idx_to_char,target_char_to_idx, target_idx_to_char = build_indices(input_characters,target_characters)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypv-dqp71uR"
      },
      "source": [
        "# now let us build datastructure based on the metadata information we obtained"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-gcs7CI8LNW",
        "outputId": "2e1c0447-86e2-4bab-ee9a-afc4ecdfb406"
      },
      "source": [
        "len(input_datasets)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jJaFul78o95",
        "outputId": "1ac8d3d3-f5a6-46dd-dc41-71a8454126c5"
      },
      "source": [
        "def build_data_structure(len_input_dataset, max_Encoder_Sequence_len, max_Decoder_Sequence_len, num_Encoder_tokens, num_Decoder_tokens):\n",
        "  Encoder_input_data = np.zeros((len_input_dataset,max_Encoder_Sequence_len,num_Encoder_tokens),dtype='float32')\n",
        "  Decoder_input_data = np.zeros((len_input_dataset,max_Decoder_Sequence_len,num_Decoder_tokens),dtype='float32')\n",
        "  Decoder_target_data = np.zeros((len_input_dataset,max_Decoder_Sequence_len,num_Decoder_tokens),dtype='float32')\n",
        "\n",
        "  print('Dimensionality of encoder input data is: ',Encoder_input_data.shape)\n",
        "  print('Dimensionality of Decoder input data is: ', Decoder_input_data.shape)\n",
        "  print('Dimensionality of Decoder target data is: ', Decoder_target_data.shape)\n",
        "\n",
        "  return Encoder_input_data, Decoder_input_data, Decoder_target_data\n",
        "Encoder_input_data, Decoder_input_data, Decoder_target_data = build_data_structure(len(input_datasets),max_Encoder_Sequence_len, max_Decoder_Sequence_len, num_Encoder_tokens, num_Decoder_tokens)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensionality of encoder input data is:  (140000, 117, 27)\n",
            "Dimensionality of Decoder input data is:  (140000, 58, 29)\n",
            "Dimensionality of Decoder target data is:  (140000, 58, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojQICA_z-7V1"
      },
      "source": [
        "The dimensionality of the input data is (140000, 117, 27):\n",
        "The first dimension caters to the number of data points we have:\n",
        "140,000.\n",
        "The second dimension caters to the maximum length of our input sequence: 117.\n",
        "The third dimension caters to the number of unique inputs we can have or the size of our input character set: 27."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jp1NibP-wdQ"
      },
      "source": [
        "# now that we have our datastructure ready let us add some data to our datastructure"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftCGGGE_DAl"
      },
      "source": [
        "def add_data_to_data_structure(input_datasets, target_datasets, Encoder_input_data, Decoder_input_data, Decoder_target_data):\n",
        "  for i, (input_datapoint,target_datapoint) in enumerate(zip(input_datasets,target_datasets)):\n",
        "    for t, char in enumerate(input_datapoint):\n",
        "      Encoder_input_data[i,t,input_char_to_idx[char]]=1\n",
        "    for t, char in enumerate(target_datapoint):\n",
        "      Decoder_input_data[i,t,target_char_to_idx[char]]=1\n",
        "      # since decoder target data is ahead of decoder input data by one timestamp\n",
        "      if t>0: #  when building the decoder target data, we do not include anything for the <start> token\n",
        "        Decoder_target_data[i,t-1,target_char_to_idx[char]] = 1\n",
        "  return Encoder_input_data, Decoder_input_data, Decoder_target_data\n",
        "Encoder_input_data, Decoder_input_data, Decoder_target_data= add_data_to_data_structure(input_datasets,target_datasets,Encoder_input_data, Decoder_input_data, Decoder_target_data)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF6jOtMLA4Ms"
      },
      "source": [
        "# Our decoder target data is the same as the decoder input data, except that it is offset by one timestep."
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNRu5McBR6F"
      },
      "source": [
        "# defining hyperparameters\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 100\n",
        "latent_dim = 256"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24_9IWljD7xk"
      },
      "source": [
        "# let us bring the Encoder into existence\n",
        "# The encoder's job is to provide a context vector where it captures the context or thought in the input sentence."
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416q4tBsEAUQ"
      },
      "source": [
        "Encoder_inputs = Input(shape=(None,num_Encoder_tokens))\n",
        "Encoder = LSTM(latent_dim, return_state=True) # decoder returns us the last hidden state and memory, which will form the context vector\n",
        "Encoder_outputs, state_h, state_c = Encoder(Encoder_inputs)\n",
        "Encoder_states = [state_h,state_c]  # context_vector "
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSzUoyAfFI4J"
      },
      "source": [
        "The encoder learns from the performance of the decoder, which happens further down the line. The decoder's error flows back and that's how the backpropagation in the encoder works and it learns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6G-kILFHiI"
      },
      "source": [
        "## let us define the decoder"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIp5X1QYFSU5"
      },
      "source": [
        "Decoder_inputs = Input(shape=(None, num_Decoder_tokens))\n",
        "Decoder_LSTM = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "# return_sequences, want an output from the decoder at every timestep and that is why we set this parameter to True.\n",
        "Decoder_outputs,_,_ = Decoder_LSTM(Decoder_inputs, initial_state= Encoder_states)\n",
        "Decoder_dense = Dense(num_Decoder_tokens,activation='softmax')\n",
        "Decoder_outputs = Decoder_dense(Decoder_outputs) # prob vector of length 29\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jARfc7XMGfw9"
      },
      "source": [
        "during training, the decoder is provided both the input data and the target data and is asked to predict the input data with an offset of 1. This helps the decoder to understand, given a context vector from the encoder, what it should be predicting. This method of learning is referred to as teacher forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-9tug0LGZy1"
      },
      "source": [
        "model = Model(inputs=[Encoder_inputs,Decoder_inputs], outputs=Decoder_outputs)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48CQX3AeGgL0",
        "outputId": "d9f465ce-6dd9-4b0d-8497-2ce75c971ca9"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None, 27)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None, 29)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 256), (None, 290816      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  292864      input_4[0][0]                    \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 29)     7453        lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 591,133\n",
            "Trainable params: 591,133\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZWiQnY-H1jE"
      },
      "source": [
        "model.fit([Encoder_input_data,Decoder_input_data], Decoder_target_data,batch_size=batch_size, epochs=epochs,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqOMW9kxpJ43"
      },
      "source": [
        "model.save('LangTras_fr_en.h5')"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmf5KzqLIP_O"
      },
      "source": [
        "## define encoder and decoder model for inferencing"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyq_wLSGWiwn",
        "outputId": "a832c72b-e3fc-4048-85e3-5039113370ae"
      },
      "source": [
        "Encoder_model = Model(Encoder_inputs,Encoder_states) # initially encoder takes input and contex vector\n",
        "\n",
        "# these are the initial input to decoder model\n",
        "Decoder_state_input_c = Input(shape=(latent_dim,)) # since LSTM ouputs vector of length 256\n",
        "Decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "Decoder_states_input = [Decoder_state_input_h, Decoder_state_input_c]  # merge to form context vector\n",
        "\n",
        "Decoder_outputs, state_h, state_c = Decoder_LSTM(Decoder_inputs,initial_state= Decoder_states_input)\n",
        "\n",
        "print(state_h.shape)\n",
        "Decoder_states = [state_h, state_c]\n",
        "Decoder_ouputs = Decoder_dense(Decoder_outputs) # prob vector of length 29\n",
        "print(Decoder_ouputs.shape)\n",
        "print(Decoder_states)\n",
        "\n",
        "Decoder_model = Model([Decoder_inputs]+Decoder_states_input,\n",
        "                      [Decoder_outputs]+Decoder_states)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n",
            "(None, None, 29)\n",
            "[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_3')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'lstm_3')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hskMO0AYkie"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  state_value = Encoder_model.predict(input_seq)\n",
        "  target_seq = np.zeros((1,1,num_Decoder_tokens))\n",
        "  target_seq[0,0,target_char_to_idx['\\t']]=1 # vector representation of start of sequence\n",
        "\n",
        "  stop_cond = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  while not stop_cond:\n",
        "    output_tokens, h,c = Decoder_model.predict([target_seq]+state_value)\n",
        "    print(output_tokens)\n",
        "    print(output_tokens.shape)\n",
        "    sampled_token_index = np.argmax(output_tokens[0,-1,:]) # see which character index has max prob of occurence\n",
        "    print(sampled_token_index)\n",
        "    sampled_char = target_idx_to_char[sampled_token_index]\n",
        "    decoded_sentence+=sampled_char\n",
        "\n",
        "    if (sampled_char=='\\n' or len(decoded_sentence)>max_Decoder_Sequence_len):\n",
        "      stop_cond = True\n",
        "    \n",
        "    target_seq = np.zeros((1,1,num_Decoder_tokens))\n",
        "    target_seq[0,0,sampled_token_index] = 1\n",
        "    state_value = [h,c]\n",
        "  \n",
        "  return decoded_sentence\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o-_cfJMckoc"
      },
      "source": [
        "# let us decode \n",
        "\n",
        "\n",
        "def decode(seq_index):\n",
        "  input_seq = Encoder_input_data[seq_index:seq_index+1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(\"::\")\n",
        "\n",
        "  print('Input sentence: ', input_datasets[seq_index])\n",
        "  print('Decoded sentence: ', decoded_sentence)\n",
        "  "
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eLYOsBGSdebQ",
        "outputId": "6dc07c06-3e72-4bc9-ffe8-e85ca6c42a77"
      },
      "source": [
        "decode(100)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[-8.79764020e-01 -9.13472056e-01 -6.66511655e-02  5.94963312e-01\n",
            "   -5.70606589e-02 -6.94866627e-02 -2.55385280e-01  5.40829360e-01\n",
            "   -4.18999642e-01 -2.68171757e-01  8.49566340e-01  8.20760489e-01\n",
            "   -9.80789363e-01  8.92872810e-02  8.89609694e-01 -1.77664340e-01\n",
            "   -4.11141843e-01 -2.54551291e-01 -6.01851521e-03 -2.32506990e-01\n",
            "   -2.95446903e-01  8.70186627e-01 -5.36578119e-01  9.93433714e-01\n",
            "    9.87477779e-01 -7.97906160e-01 -9.33349252e-01 -6.78995192e-01\n",
            "    3.40876788e-01 -3.19709405e-02  5.55941761e-01 -1.26072377e-01\n",
            "    9.84864831e-01  2.63091266e-01 -5.02285123e-01  8.78601894e-02\n",
            "   -2.96545893e-01 -7.09043071e-02 -6.38931453e-01  7.50173926e-02\n",
            "    9.68646884e-01  3.15000534e-01 -5.10170281e-01 -1.89677268e-01\n",
            "   -7.30107054e-02 -9.97550189e-01  8.02389026e-01 -9.81229663e-01\n",
            "    5.30606925e-01 -2.28877679e-01 -1.08366348e-01 -6.12667799e-01\n",
            "   -7.26666212e-01  7.93497622e-01  7.55149007e-01 -8.41312826e-01\n",
            "   -1.31731282e-03 -7.03657150e-01  1.83979601e-01  3.95318903e-02\n",
            "    5.69245312e-03  2.82915235e-01  3.82118315e-01  3.23455662e-01\n",
            "   -5.85173786e-01  7.12335348e-01  4.26356554e-01 -9.28942502e-01\n",
            "   -5.68657704e-02  5.60629368e-02 -7.43027270e-01 -8.02889109e-01\n",
            "    7.39263222e-02 -7.52030432e-01 -9.21057258e-03 -1.47099882e-01\n",
            "   -6.35127902e-01 -2.50975657e-02 -7.32584596e-01  6.70506120e-01\n",
            "    4.36244905e-01  1.63524020e-02 -5.58398783e-01  2.69124031e-01\n",
            "   -2.86252737e-01 -3.39300364e-01 -6.89788461e-01  6.93806648e-01\n",
            "    1.32537251e-02  1.44686252e-01 -1.41699808e-02 -2.60739803e-01\n",
            "   -6.06071770e-01  4.77635980e-01  5.68822682e-01 -7.56206274e-01\n",
            "    9.98671532e-01 -4.67489213e-01  9.85352516e-01 -4.30151820e-02\n",
            "    4.69721891e-02 -1.50516583e-03 -6.00276232e-01 -5.55898011e-01\n",
            "   -1.87071171e-02  7.33288705e-01  5.93260629e-03 -1.99154407e-01\n",
            "    5.59972942e-01 -6.40169680e-01 -6.85366452e-01 -6.04844630e-01\n",
            "    5.99355474e-02  2.40392402e-01 -1.26964092e-01 -9.23073590e-01\n",
            "   -7.06312001e-01 -4.59580608e-02  1.18864970e-02 -2.18680367e-01\n",
            "   -6.07876554e-02  7.52778530e-01 -1.09910354e-01 -8.32043350e-01\n",
            "    8.61249268e-01 -9.98939335e-01  6.19036019e-01  5.03717847e-02\n",
            "   -6.44307852e-01 -9.61561680e-01  1.52639121e-01 -1.07215866e-01\n",
            "    6.51237904e-04 -3.03973025e-03 -5.38892066e-03 -5.47503168e-03\n",
            "   -1.69158369e-01  8.78588185e-02 -6.79861009e-01 -3.65021639e-03\n",
            "   -2.20104918e-01 -1.00819834e-01  2.79038578e-01 -7.87033021e-01\n",
            "    6.25897288e-01  1.04544759e-01  5.31716347e-01  7.44672418e-01\n",
            "    5.33003688e-01 -1.85280725e-01  7.43093669e-01  5.36886603e-02\n",
            "   -2.16852548e-03  9.71015811e-01 -8.76486838e-01 -1.75132364e-01\n",
            "    1.56712413e-01 -7.90357947e-01  2.69734025e-01 -2.18280014e-02\n",
            "    5.43169864e-02 -6.22807324e-01 -6.72471225e-01 -5.89434683e-01\n",
            "    1.85383216e-03 -9.99189198e-01  1.37981907e-01  1.67274326e-01\n",
            "   -5.88678837e-01 -2.10475862e-01 -2.63102710e-01 -3.17873694e-02\n",
            "   -4.35632654e-02 -4.30259228e-01  4.15181488e-01 -7.31427491e-01\n",
            "    8.36091757e-01  7.89009690e-01  6.79336786e-01 -9.79585171e-01\n",
            "    6.40007913e-01 -1.58139706e-01 -7.43284047e-01 -3.69544923e-01\n",
            "    1.65090546e-01 -1.72550470e-01 -9.15207207e-01  6.50829494e-01\n",
            "   -9.93730366e-01  1.33409142e-01  3.18149060e-01  9.33529377e-01\n",
            "    6.32696569e-01 -8.33422244e-02  6.12692654e-01  4.27892715e-01\n",
            "    1.78449988e-01 -4.08361226e-01 -1.15176320e-01  7.06550658e-01\n",
            "    9.94078159e-01 -6.19465947e-01 -5.01641691e-01 -8.82638037e-01\n",
            "   -5.83125651e-01  7.90332407e-02  4.26175773e-01 -6.12277091e-01\n",
            "    9.10564184e-01 -4.56556715e-02  5.72034001e-01 -9.82532650e-02\n",
            "   -4.47852276e-02  8.83897319e-02  9.88796830e-01  1.99279701e-03\n",
            "   -6.97827160e-01  1.35449305e-01  9.80472565e-02  1.83710873e-01\n",
            "    2.81585485e-01 -6.56921446e-01  2.96315104e-01 -7.43966699e-01\n",
            "   -5.15322462e-02 -2.49680847e-01  7.03516006e-01 -7.38230228e-01\n",
            "   -1.18217312e-01  1.56562865e-01 -3.63073719e-04 -6.34864748e-01\n",
            "   -1.09838555e-02 -9.46332932e-01  1.87310576e-03 -5.76612175e-01\n",
            "    2.62602329e-01 -4.26459908e-01 -9.98910666e-01  6.62607014e-01\n",
            "   -8.97706807e-01 -1.91371646e-02 -1.04073212e-01 -2.50150770e-01\n",
            "    8.67087066e-01  1.56557485e-01 -3.94728966e-02 -3.26224357e-01\n",
            "   -7.47329593e-01  3.60133429e-03 -9.66773331e-01  5.67854829e-02\n",
            "    3.12582374e-01 -5.98772585e-01 -6.92121983e-01  5.71740746e-01]]]\n",
            "(1, 1, 256)\n",
            "96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-ae1af154758b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-120-9ad3ff86332c>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(seq_index)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-119-a98c861a8268>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# see which character index has max prob of occurence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msampled_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_idx_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0msampled_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 96"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4bMCTQUpVbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}