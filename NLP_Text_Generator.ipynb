{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Text_Generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKY4romC4+qYLWRSJ3Dz6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush360/Natural_langauge_processing/blob/main/NLP_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZkPpHu3d84Z"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Dropout,Embedding"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDH5gw7keEqV",
        "outputId": "20025a09-3135-4b6a-de7e-0da1fea5a214"
      },
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JYnmBFXeHI3"
      },
      "source": [
        "df_bkp = pd.read_csv('DataSetNew.txt',sep=\"\\n\", header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LDDXv5Lied9m",
        "outputId": "45fa89f5-8f5f-4c0a-a4b6-5bdfcb66103f"
      },
      "source": [
        "df_bkp.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hello</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hi, how are you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hello, welcome to naulo restaurant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hey</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    0\n",
              "0                               hello\n",
              "1                     hi, how are you\n",
              "2                                  hi\n",
              "3  hello, welcome to naulo restaurant\n",
              "4                                 hey"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Jlq9sLjhefxR",
        "outputId": "d180288f-1c91-45f8-f628-c8f92cfce13f"
      },
      "source": [
        "df_bkp.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>santosh shah who dedicated his life to cooking...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>what restaurant does santosh shah saha work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>santosh shah works in cinnamon kitchen as head...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>what are the upcoming events of santosh shah saha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>there are no upcoming events of santosh shah a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0\n",
              "565  santosh shah who dedicated his life to cooking...\n",
              "566        what restaurant does santosh shah saha work\n",
              "567  santosh shah works in cinnamon kitchen as head...\n",
              "568  what are the upcoming events of santosh shah saha\n",
              "569  there are no upcoming events of santosh shah a..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CptX2OfeGO"
      },
      "source": [
        "data = df_bkp.copy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7lWqDwcf5U3"
      },
      "source": [
        "data.columns=['Sentences']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktF92hmAgA2N",
        "outputId": "3bec49be-e960-4cb1-8dcc-2ee99cd0663f"
      },
      "source": [
        "data['Sentences']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                  hello\n",
              "1                                        hi, how are you\n",
              "2                                                     hi\n",
              "3                     hello, welcome to naulo restaurant\n",
              "4                                                    hey\n",
              "                             ...                        \n",
              "565    santosh shah who dedicated his life to cooking...\n",
              "566          what restaurant does santosh shah saha work\n",
              "567    santosh shah works in cinnamon kitchen as head...\n",
              "568    what are the upcoming events of santosh shah saha\n",
              "569    there are no upcoming events of santosh shah a...\n",
              "Name: Sentences, Length: 570, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMzXFOprev-2"
      },
      "source": [
        "## Pre-Process the data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUr0D61EfbFR"
      },
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "def stopword_removal(data_point):\n",
        "  data= [x for x in data_point.split() if x not in stop]\n",
        "  return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jikiW1hyfnD1"
      },
      "source": [
        "def clean_data(data): \n",
        "  '''works for single sentence and returns cleaned tokens of words in the sentence along with unique_words'''\n",
        "  cleaned_data =[]\n",
        "  all_unique_words_in_each_description=[]\n",
        "  for entry in data:\n",
        "    entry = re.sub(pattern='[^a-zA-Z]',repl=\" \", string=entry) # check if alphabet, remove non-alphabet\n",
        "    entry = re.sub(r'\\b\\w{0,1}\\b', repl=\" \", string=entry)  # remove single word or blank character/word\n",
        "    entry =entry.lower()\n",
        "    entry = stopword_removal(entry)\n",
        "    cleaned_data.append(entry) # list with in list 0(1) time-complexity\n",
        "    unique = list(set(entry))\n",
        "    all_unique_words_in_each_description.extend(unique) # appended list O(K) time-complexity, k- lenght of list need to be added\n",
        "  return cleaned_data,all_unique_words_in_each_description"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvHNcBuGfq4j"
      },
      "source": [
        "# function to make vocabulary\n",
        "\n",
        "def unique_words(data):\n",
        "  unique_words = set(data)\n",
        "  return unique_words, len(unique_words)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRo4bZdftku"
      },
      "source": [
        "## applying data cleaning"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRgYaEijfx_W"
      },
      "source": [
        "cleaned_data, all_unique_words_in_each_description = clean_data(data['Sentences'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYO3Eedifz6G"
      },
      "source": [
        "unique_words, len_unique_words = unique_words(all_unique_words_in_each_description)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNei1DnKgeUX"
      },
      "source": [
        "unique_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh6sa68RgLIf",
        "outputId": "9ccd1a62-7a6f-4dc4-970a-8dc720d619cc"
      },
      "source": [
        "len_unique_words  # this is our vocab_size\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn4r2NuWgNHj",
        "outputId": "8bc295d2-f692-495b-8169-c66a2b31391d"
      },
      "source": [
        "\n",
        "## peeking at one of the cleaned data\n",
        "\n",
        "len(cleaned_data[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb2aGsr3gPk1",
        "outputId": "766ec160-5e08-423c-bdb3-c9a27fc25e98"
      },
      "source": [
        "cleaned_data[3]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'welcome', 'naulo', 'restaurant']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVa0iw5pgRiz"
      },
      "source": [
        "## Building a Mapper"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEXf65CVgXTa"
      },
      "source": [
        "def build_indices(unique_words):\n",
        "  word_to_index = {}\n",
        "  index_to_word = {}\n",
        "  for i,word in enumerate(unique_words):\n",
        "    word_to_index[word]=i\n",
        "    index_to_word[i]=word\n",
        "  return word_to_index, index_to_word"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sovnqYXSgZaM"
      },
      "source": [
        "word_to_index, index_to_word = build_indices(unique_words)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFayW-3ogcov"
      },
      "source": [
        "## prepare training corpus"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHtEqBy7gleU"
      },
      "source": [
        "def prepare_corpus(corpus,word_to_index):\n",
        "  sequences= []\n",
        "  for line in corpus:\n",
        "    tokens = line\n",
        "    for i in range(1, len(tokens)):\n",
        "      i_gram_sequence = tokens[:i+1]\n",
        "      i_gram_sequence_ids = []\n",
        "\n",
        "      for j,token in enumerate(i_gram_sequence):\n",
        "        i_gram_sequence_ids.append(word_to_index[token])\n",
        "      sequences.append(i_gram_sequence_ids)\n",
        "  return sequences"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2O-F556gpPR"
      },
      "source": [
        "sequences=prepare_corpus(cleaned_data,word_to_index)\n",
        "max_sequence_len = max([len(x) for x in sequences]) # find sentence with maximum length"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ok5-hqtgvkj",
        "outputId": "4af25f2a-9163-4abd-effc-8dd1567e8816"
      },
      "source": [
        "len(sequences) # our sequences contains 2828 list with colleciton of n-grams for each sentence staritng from length 2\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRf82NEHgxon",
        "outputId": "39d15353-fa3a-47e7-a310-b26ddd7b7c3c"
      },
      "source": [
        "max_sequence_len # so the max len of sentece after cleaning and tokenizing is 308\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTRAhaC2g6Qg",
        "outputId": "63270d75-6830-4024-e238-b95fec58d2ff"
      },
      "source": [
        "sequences[0] # looking at the first sentence and its i-gram indices (first sentence with 2 words indexes)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[75, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQEjo1zLg8WI",
        "outputId": "2177a317-bb34-47be-acaf-592e76c4164e"
      },
      "source": [
        "sequences[1]\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[75, 102, 89]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lawrWbDTg_cF",
        "outputId": "1c798087-de2e-4511-b541-4f398e3ec062"
      },
      "source": [
        "# let us see which words are mapped to these indices\n",
        "\n",
        "print(index_to_word[640])\n",
        "print(index_to_word[286])\n",
        "print(index_to_word[733])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "putting\n",
            "assist\n",
            "mexicanna\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4bo28fvhE8a"
      },
      "source": [
        "# Finally, we have correctly build our sequences\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clrQNHqAhHwo"
      },
      "source": [
        "\n",
        "## Preparing the data for trainng the model\n",
        "# we should pad the input to the longest words in the sequences 38 in this case\n",
        "# also we need to split the data into dependent and independent variable"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbRRRcn7hM-1"
      },
      "source": [
        "def build_input_data(sequences,max_sequence_len, len_unique_words):\n",
        "  sequences = np.array(pad_sequences(sequences,maxlen=max_sequence_len, padding='pre'))\n",
        "  X = sequences[:,:-1]\n",
        "  y = sequences[:,-1]\n",
        "  y = np_utils.to_categorical(y,len_unique_words) # convert correspoding row index to 1, and len of each word is vector of length len_unique_words\n",
        "  return X,y"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4okctpbUhPi9"
      },
      "source": [
        "X,y = build_input_data(sequences,max_sequence_len,len_unique_words)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmSrTnbIhSx0"
      },
      "source": [
        "### Building the model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pWzLvzFhXj1"
      },
      "source": [
        "input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words. output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem. input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgd7gmnFhWYj"
      },
      "source": [
        "# 10 indicates that we want a Dense embedding of size 10 as output from our model.\n",
        "# 128 LSTM units in the hidden layer, dimensionality of the inner cells in the LSTM layer.\n",
        "# randomly drop off 20% of neurons from the network using the Dropout layer.\n",
        "#  multi-class classification problem, and so the softmax activation function is used."
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwW1UWdnhY9C"
      },
      "source": [
        "\n",
        "def create_model(max_sequence_len, len_unique_words):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len_unique_words,10,input_length=max_sequence_len-1))\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(len_unique_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTBTWlnhY_x",
        "outputId": "404784e7-5535-4b23-92f9-d1a8ce898012"
      },
      "source": [
        "\n",
        "model = create_model(max_sequence_len, len_unique_words)\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 37, 10)            8470      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               71168     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 847)               109263    \n",
            "=================================================================\n",
            "Total params: 188,901\n",
            "Trainable params: 188,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLvzC1LkhZCO"
      },
      "source": [
        "model.fit(X,y,batch_size=128, epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kavn_MQUh-Sq"
      },
      "source": [
        "## testing and text generation"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGvgOO5XiNtM"
      },
      "source": [
        "\n",
        "def generate_text(seed_text, next_words, model, max_seq_len):\n",
        "  for _ in range(next_words):\n",
        "    cleaned_data = clean_data([seed_text])\n",
        "    sequences = prepare_corpus(cleaned_data[0],word_to_index)\n",
        "    sequences = pad_sequences([sequences[-1]],maxlen=max_seq_len-1,padding='pre')\n",
        "    predicted = model.predict_classes(sequences, verbose=0)\n",
        "    output_word=''\n",
        "    output_word = index_to_word[predicted[0]]\n",
        "    seed_text = seed_text+\" \"+output_word\n",
        "  return seed_text.title()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjFPEH0uiOXa",
        "outputId": "5db1f3b8-2ef1-4c73-b280-03e22739262d"
      },
      "source": [
        "print(generate_text(\"Hello do you like\",8,model,max_sequence_len))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hello Do You Like Suggest Favorite Cheese Brust Pizza Done Pizza Thank\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5XiwptGiVxR"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVZcrOoGjJDT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}